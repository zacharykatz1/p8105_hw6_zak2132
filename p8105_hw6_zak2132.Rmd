---
title: "P8105 Homework #6"
author: 'Zachary Katz (UNI: zak2132)'
date: "12/4/2021"
output: 
  github_document:
    toc: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# Load packages
library(tidyverse)
library(viridis)
library(readxl)
library(readr)
library(modelr)
library(purrr)
library(patchwork)
library(mgcv)
library(leaps)

# Set global options for embedding plots and choosing themes
knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)
theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1

```{r}
# Load and clean the data
baby_df = 
  # Load in the data and clean column names to be more descriptive
  read_csv("./data/birthweight.csv") %>% 
  janitor::clean_names() %>% 
  rename(
    momwt = delwt,
    momrace = mrace,
    dadrace = frace,
    avg_daily_cigs = smoken,
    momheight = mheight
  ) %>% 
  # Recode categoricals as factor variables with descriptive levels
  mutate(
    dadrace = 
      case_when(
        dadrace == 1 ~ "White",
        dadrace == 2 ~ "Black",
        dadrace == 3 ~ "Asian",
        dadrace == 4 ~ "Puerto Rican",
        dadrace == 8 ~ "Other",
        dadrace == 9 ~ "Unknown",
        TRUE ~ ""
      ) %>% as.factor(),
    momrace = 
      case_when(
        momrace == 1 ~ "White",
        momrace == 2 ~ "Black",
        momrace == 3 ~ "Asian",
        momrace == 4 ~ "Puerto Rican",
        momrace == 8 ~ "Other",
        TRUE ~ ""
      ) %>% as.factor(),
    malform =
      case_when(
        malform == 0 ~ "Absent",
        malform == 1 ~ "Present"
      ) %>% as.factor(),
    babysex = 
      ifelse(
        babysex == 1, "Male", "Female"
        ) %>% as.factor(),
    # Convert units for consistency (to grams and centimeters)
    momwt_grams = momwt * 453.592,
    momheight_cm = momheight * 2.54) %>% 
  # Rename columns with units where applicable and remove no longer necessary variables
  rename(
    bwt_grams = bwt,
    wtgain_grams = wtgain,
  ) %>% 
  select(-momheight, -momwt)

# Check for missingness -- no missing values!
baby_df %>% 
  skimr::skim() %>% 
  select(skim_variable, n_missing, complete_rate) %>% 
  knitr::kable()
```

After loading the data, the following steps were taken to clean and tidy the tibble:

* Renaming variables to be more accurate and descriptive
* Converting relevant variables to factors with appropriate levels
* Ensuring consistency of units (grams and centimeters rather than pounds and inches for weight and height, respectively)
* Checking for missingness; all variables havea 100% completeness rate

``` {r}
# Explore head of data frame
baby_df %>% 
  head(10) %>% 
  knitr::kable()

# Check structure of data frame
str(baby_df)

# Explore summary of data frame
skimr::skim(baby_df)
```

In total, there are `r nrow(baby_df)` observations and `r ncol(baby_df)` variables in `baby_df`, where each observation represents one baby birth. We also note that `pnumlbw` and `pnumgsa` are both 0 across all observations, so we are unlikely to use these in future regression analysis.

#### Propose a Regression Model for Birthweight

First, let's check that a linear model is appropriate by ensuring our data meets the normality assumption:

```{r}
# Check normality assumption by plotting birthweight distribution
baby_df %>% 
  ggplot(aes(x = bwt_grams)) + 
  geom_histogram(binwidth = 30) + 
  labs(
    title = "Birthweight Distribution",
    x = "Birthweight (g)",
    y = "Count"
  )
```

Birthweight appears roughly normally distributed, with mean of `r mean(pull(baby_df, bwt_grams))` grams and SD of `r sd(pull(baby_df, bwt_grams))`.

Although unsure yet which continuous predictors we'll use, we can choose a couple of potential ones to check the linearity assumption for fun as well:

```{r}
bwt_vs_momwt = 
  baby_df %>% 
  ggplot(aes(x = bwt_grams, y = momwt_grams)) + 
  geom_smooth(se = FALSE) + 
  geom_point(alpha = 0.05) + 
  labs(
    title = "Association between Baby Birthweight and Mom's Weight",
    x = "Baby's Weight (g)",
    y = "Mom's Weight (g)"
  )

bwt_vs_momht = 
  baby_df %>% 
  ggplot(aes(x = bwt_grams, y = momheight_cm)) + 
  geom_smooth(se = FALSE) + 
  geom_point(alpha = 0.05) + 
  labs(
    title = "Association between Baby Birthweight and Mom's Height",
    x = "Baby's Weight (g)",
    y = "Mom's Height (cm)"
  )

bwt_vs_momwt + bwt_vs_momht
```

We find a bend in the curve for both graphs at around baby's weight of 2500 grams. That said, we can proceed for now with linear regression assuming we meet the linearity assumption, but may want to be cautious going forward after this finding.

One way to approach model development is to hypothesize relevant covariates a priori, and then use the "best subset" procedure to select the ideal subset of covariates from our hypothesized superset based on the maximization of adjusted R-squared and minimization of BIC and Cp. 

Let's begin by fitting a preliminary model using covariates we believe best predict birth weight. First, we exclude variables due to low cell counts or fewer than two levels, including `malform`, `pnumlbw`, and `pnumgsa`. Then, we can explore the literature on low birth weight, where we find some known associations with birth weight; for example, smoking is known to be associated with lower birth weight. Prior findings of significant association with birthweight include parental race, baby's sex, baby's head circumference and length at birth, gestational age, maternal height and age, and mother's weight gain during pregnancy. We develop our preliminary model as follows

```{r}
# Proposed preliminary linear model
prelim_model = 
  lm(
    bwt_grams ~ babysex + bhead + blength + dadrace + gaweeks + momheight_cm + momage + momrace + avg_daily_cigs + wtgain_grams,
    data = baby_df
  )

# Develop a clean summary table and examine which covariates have p < 0.05
summary(prelim_model) %>% 
  broom::tidy() %>% 
  mutate(
    p_value = format.pval(p.value, digits = 2, eps = 0.05)
  ) %>% 
  select(-p.value) %>%
  arrange(p_value) %>% 
  knitr::kable()
```

A quick first glance at the data tells us that some promising covariates of the ones included in our preliminary model are `babysex`, `babyhead`, `babylength`, `gaweeks`, `momheight_cm`, `avg_daily_cigs`, and `wtgain_grams`, which all have p-values under 0.05.

With this initial model, we can use the "best subset" method to determine the ideal set of covariates to include in a more final model.

```{r}
# Limit data frame to covariates from preliminary model
prelim_covariates = 
  baby_df %>% 
  select(bwt_grams, babysex, bhead, blength, dadrace, gaweeks, momheight_cm , momage, momrace, avg_daily_cigs, wtgain_grams)

# Run regsubsets for best subset model across preliminary covariates
best_subset_model = regsubsets(
  bwt_grams ~ ., 
  data = prelim_covariates
  )

# Save best subset summary as object
best_subset_summary = summary(best_subset_model)

# Save summary stats for each subset model to data frame
stats_summary_subsets = data.frame(
  adjusted_r2 = best_subset_summary$adjr2,
  cp = best_subset_summary$cp,
  bic = best_subset_summary$bic
)

# Bind with variables used in each model for easy review
subset_models =
  as.data.frame(best_subset_summary$outmat) %>% 
  cbind(stats_summary_subsets) %>% 
  arrange(desc(adjusted_r2), cp, bic)

# Print summary overview of best subsets
subset_models %>% 
  knitr::kable(
    digits = 2
  )
```

We select the best model of the subsets by maximizing adjusted R2 and minimizing prediction error (cp and BIC in this case). The model that best achieves these goals incorporates `babysex`, `bhead`, `blength`, `gaweeks`, `momheight_cm`, `momrace`, `avg_daily_cigs`, and `wtgain_grams`. This means we exclude `dadrace` and `momage` from our final model.

We run our final model as follows, and then show a plot of model residuals against fitted values:

```{r}
# Run final model using optimized set of covariates
final_model = lm(
  bwt_grams ~ babysex + bhead + blength + gaweeks + momheight_cm + momrace + avg_daily_cigs + wtgain_grams,
  data = baby_df
)

# Print tidied summary of final model
summary(final_model) %>% 
  broom::tidy() %>% 
  mutate(
    p_value = format.pval(p.value, digits = 2, eps = 0.05)
  ) %>% 
  select(-p.value) %>%
  arrange(p_value) %>% 
  knitr::kable(
    digits = 2
  )

# Plot model residuals against fitted values
baby_df %>% 
  add_predictions(final_model) %>% 
  add_residuals(final_model) %>% 
  ggplot(aes(x = pred, y = resid)) + 
  geom_point(alpha = 0.2) + 
  geom_smooth(se = FALSE) + 
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") + 
  labs(
    title = "Residuals against Fitted Values for Final Birthweight Linear Model",
    x = "Fitted Values",
    y = "Residuals"
  )
```

We note that there are a couple of babies with predicted birthweight less than 0, but recognize that they are outliers. Most residuals appear roughly evenly dispersed across predicted (fitted) values, meaning that except for the lower end of our fitted values, we tend to see constant variance. However, the fact that residuals are not scattered around 0 on the low end of our fitted values indicates there may be some violation of the assumption of constant variance.

#### Model Comparisons

We need to compare our model to two others:

* A model that uses length at birth and gestational age as predictors (main effects only)
* A model that uses head circumference, length, sex, and all interactions (including the three-way interaction) between these

```{r}
# Develop model using length at birth and gestational age
baby_df %>% 
  lm(bwt_grams ~ blength + gaweeks, data = .) %>% 
  broom::tidy() %>% 
  knitr::kable(
    digits = 2
  )

# Develop model using head circumference, length, sex, and interactions
baby_df %>% 
  lm(bwt_grams ~ bhead + blength + babysex + bhead*blength + bhead*babysex + blength*babysex + bhead*blength*babysex, data = .) %>% 
  broom::tidy() %>% 
  knitr::kable(
    digits = 2
  )
```

Now, we can compare our models using cross-validated prediction error:

```{r}
# Create cross-validation datasets
# Split original sample into 80% training, 20% testing, with 100 resamples
cv_df = 
  crossv_mc(baby_df, 100) %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)
  )

# Fit each of the three models on training data
# Then, compute RMSE on testing data
cv_df = 
  cv_df %>% 
  mutate(
    my_model = map(.x = train, ~lm(bwt_grams ~ babysex + bhead + blength + gaweeks + momheight_cm + momrace + avg_daily_cigs + wtgain_grams, data = .x)),
    model2 = map(.x = train, ~lm(bwt_grams ~ blength + gaweeks, data = .x)),
    model3 = map(.x = train, ~lm(bwt_grams ~ bhead + blength + babysex + bhead*blength + bhead*babysex + blength*babysex + bhead*blength*babysex, data = .x))
  ) %>% 
  mutate(
    rmse_my_model = map2_dbl(.x = my_model,.y = test, ~rmse(model = .x, data = .y)),
    rmse_model2 = map2_dbl(.x = model2,.y = test, ~rmse(model = .x, data = .y)),
    rmse_model3 = map2_dbl(.x = model3,.y = test, ~rmse(model = .x, data = .y))
  )

# Data frame with RMSEs across models
rmse_df = cv_df %>% 
  select(rmse_my_model, rmse_model2, rmse_model3) %>% 
  pivot_longer(
    everything(),
    names_to = "model",
    values_to = "RMSE",
    names_prefix = "rmse_"
  )

# Print table of summary mean RMSE across models
rmse_df %>% 
  group_by(model) %>% 
  summarize(
    mean_RMSE = mean(RMSE)
  ) %>% 
  knitr::kable()
```

From the table, we observe that my model has the lowest mean RMSE across cross-validation trials. Let's plot for comparison of RMSE distributions:

```{r}
# Violin plot of RMSE distribution for each model
rmse_df %>% 
  mutate(
    # Maintain original model order
    model = fct_inorder(model)
  ) %>% 
  ggplot(aes(x = model, y = RMSE, fill = model)) + 
  geom_violin(alpha = 0.3) + 
  labs(
    title = "Comparing Prediction Error Across Models",
    x = "Model",
    y = "RMSE"
  ) + 
  scale_x_discrete(
    labels = c("My Model", "Length & Gestational Age", "Interactions Model")
  ) + 
  theme(legend.position = "none")
```

My proposed model has lower prediction error (minimized RMSE) compared to the other two models provided. The second model, which looks only at baby length and gestational age, performs the worst, while the interactions model is somewhere in the middle. I would select my model for prediction purposes, but may want to further explore whether we achieve the constant variance assumption based on the residuals vs. fitted values graph already provided. Ultimately, if we violate this assumption, I might go with the interactions model -- assuming it has better dispersion of residuals across fitted values.

## Problem 2

```{r}
# Load the data
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())

weather_df
```

#### Bootstrapping

Developing a simple linear regression with `tmax` as the response and `tmin` as the predictor, we're interested in the distribution of the estimated r-squared and log(beta_0*beta_1). First, let's bootstrap 5000 times:

```{r}
# Bootstrap 5000 times
bootstrap_df = 
  weather_df %>% 
  bootstrap(n = 5000, id = "strap_num") %>% 
  # Convert resample objects to df and clean up bootstrap results
  mutate(
    strap = map(strap, as_tibble),
    models = map(strap, ~lm(tmax ~ tmin, data = .x)),
    estimates = map(models, broom::tidy),
    summary = map(models, broom::glance)
  ) %>% 
  select(-strap, -models) %>% 
  unnest(summary)

# Bootstrap 5000 times
bootstrap_df = 
  weather_df %>% 
  bootstrap(n = 5000, id = "strap_num") %>% 
  # Convert resample objects to df and clean up bootstrap results
  mutate(
    strap = map(strap, as_tibble),
    models = map(strap, ~lm(tmax ~ tmin, data = .x)),
    estimates = map(models, broom::tidy),
    summary = map(models, broom::glance)
  ) %>% 
  select(-strap, -models) %>% 
  unnest(c(estimates, summary), names_repair = "universal") %>% 
  select(strap_num, term, estimate, r.squared) %>% 
  pivot_wider(
    id_cols = c(strap_num, r.squared),
    names_from = term,
    values_from = estimate
  ) %>% 
  rename(
    beta0 = `(Intercept)`, 
    beta1 = "tmin"
  ) %>% 
  mutate(
    logb0_b1 = log(beta0 * beta1)
  ) %>% 
  select(-beta0, -beta1)

# Check head of bootstrap df
bootstrap_df %>% 
  head(10) %>% 
  knitr::kable(
    digits = 2
  )
```

#### R-squared

Now we can extract the 95% confidence interval for the estimated r-squared when we perform a simple linear regression `tmax ~ tmin`.

```{r}
# Find 95% confidence interval for R-squared
r_squared = bootstrap_df %>% 
  select(r.squared) %>% 
  summarize(
    lower_ci = quantile(r.squared, 0.025),
    upper_ci = quantile(r.squared, 0.975)
  )

# Print table of 95% CI
r_squared %>%
  knitr::kable(
    digits = 3
  )
```

Our 95% confidence interval ranges from 0.894 to 0.927. And we can also plot the distribution of our estimated r-squared as follows:

```{r}
# Plot distribution of r-squared after bootstrapping
bootstrap_df %>% 
  select(r.squared) %>% 
  ggplot(aes(x = r.squared)) + 
  geom_density() + 
  labs(
    title = "Distribution of Estimated R-Squared",
    subtitle = "Bootstrapping 5000 times",
    x = "Estimated R Squared",
    y = "Density"
  )
```

We find from this graph that the R-squared values from bootstrapping appear normally distributed with mean `r round(mean(pull(bootstrap_df, r.squared)), 3)` and SD `r round(sd(pull(bootstrap_df, r.squared)), 3)`.

#### log(B_0 * B_1)

Similarly, we can extract the 95% confidence interval for the estimated log(beta_0 * beta_1) when we perform a simple linear regression `tmax ~ tmin`.

```{r}
# Find 95% confidence interval for log(beta0*beta1)
log_b0_b1 = bootstrap_df %>% 
  select(logb0_b1) %>% 
  summarize(
    lower_ci = quantile(logb0_b1, 0.025),
    upper_ci = quantile(logb0_b1, 0.975)
  )

# Print table of 95% CI
log_b0_b1 %>%
  knitr::kable(
    digits = 3
  )
```

And also can plot its distribution across the 5000 bootstraps:

```{r}
# Plot distribution of log(b0b1) after bootstrapping
bootstrap_df %>% 
  select(logb0_b1) %>% 
  ggplot(aes(x = logb0_b1)) + 
  geom_density() + 
  labs(
    title = "Distribution of Estimated log(beta0*beta1)",
    subtitle = "Bootstrapping 5000 times",
    x = "Estimated log(beta0*beta1)",
    y = "Density"
  )
```

Our 95% confidence interval ranges from 1.965 to 2.060. Again, it appears our estimated log(b0 * b1) is normally distributed with mean `r round(mean(pull(bootstrap_df, logb0_b1)), 3)` and SD `r round(sd(pull(bootstrap_df, logb0_b1)), 3)`.